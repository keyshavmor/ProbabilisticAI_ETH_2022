{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c854cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import important packages\n",
    "\n",
    "import os\n",
    "import typing\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import gpytorch\n",
    "from matplotlib import rcParams\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb1cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `EXTENDED_EVALUATION` to `True` in order to visualize your predictions.\n",
    "EXTENDED_EVALUATION = True\n",
    "EVALUATION_GRID_POINTS = 300  # Number of grid points used in extended evaluation\n",
    "EVALUATION_GRID_POINTS_3D = 50  # Number of points displayed in 3D during evaluation\n",
    "\n",
    "\n",
    "# Cost function constants\n",
    "COST_W_UNDERPREDICT = 25.0\n",
    "COST_W_NORMAL = 1.0\n",
    "COST_W_OVERPREDICT = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb93416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Model, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=train_x.shape[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "def make_predictions(self, test_features: np.ndarray) -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Predict the pollution concentration for a given set of locations.\n",
    "    :param test_features: Locations as a 2d NumPy float array of shape (NUM_SAMPLES, 2)\n",
    "    :return:\n",
    "        Tuple of three 1d NumPy float arrays, each of shape (NUM_SAMPLES,),\n",
    "        containing your predictions, the GP posterior mean, and the GP posterior stddev (in that order)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Use your GP to estimate the posterior mean and stddev for each location here\n",
    "    gp_mean = np.zeros(test_features.shape[0], dtype=float)\n",
    "    gp_std = np.zeros(test_features.shape[0], dtype=float)\n",
    "\n",
    "    return predictions, gp_mean, gp_std\n",
    "\n",
    "def fitting_model(model, train_GT: np.ndarray,train_features: np.ndarray):\n",
    "    \"\"\"\n",
    "    Fit your model on the given training data.\n",
    "    :param train_features: Training features as a 2d NumPy float array of shape (NUM_SAMPLES, 2)\n",
    "    :param train_GT: Training pollution concentrations as a 1d NumPy float array of shape (NUM_SAMPLES,)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Fit your model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0354f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(ground_truth: np.ndarray, predictions: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cost of a set of predictions.\n",
    "\n",
    "    :param ground_truth: Ground truth pollution levels as a 1d NumPy float array\n",
    "    :param predictions: Predicted pollution levels as a 1d NumPy float array\n",
    "    :return: Total cost of all predictions as a single float\n",
    "    \"\"\"\n",
    "    assert ground_truth.ndim == 1 and predictions.ndim == 1 and ground_truth.shape == predictions.shape\n",
    "\n",
    "    # Unweighted cost\n",
    "    cost = (ground_truth - predictions) ** 2\n",
    "    weights = np.ones_like(cost) * COST_W_NORMAL\n",
    "\n",
    "    # Case i): underprediction\n",
    "    mask_1 = predictions < ground_truth\n",
    "    weights[mask_1] = COST_W_UNDERPREDICT\n",
    "\n",
    "    # Case ii): significant overprediction\n",
    "    mask_2 = (predictions >= 1.2*ground_truth)\n",
    "    weights[mask_2] = COST_W_OVERPREDICT\n",
    "\n",
    "    # Weigh the cost and return the average\n",
    "    return np.mean(cost * weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fc0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_extended_evaluation(model: Model, output_dir: str = '/results'):\n",
    "    \"\"\"\n",
    "    Visualizes the predictions of a fitted model.\n",
    "    :param model: Fitted model to be visualized\n",
    "    :param output_dir: Directory in which the visualizations will be stored\n",
    "    \"\"\"\n",
    "    print('Performing extended evaluation')\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    fig.suptitle('Extended visualization of task 1')\n",
    "\n",
    "    # Visualize on a uniform grid over the entire coordinate system\n",
    "    grid_lat, grid_lon = np.meshgrid(\n",
    "        np.linspace(0, EVALUATION_GRID_POINTS - 1, num=EVALUATION_GRID_POINTS) / EVALUATION_GRID_POINTS,\n",
    "        np.linspace(0, EVALUATION_GRID_POINTS - 1, num=EVALUATION_GRID_POINTS) / EVALUATION_GRID_POINTS,\n",
    "    )\n",
    "    visualization_xs = np.stack((grid_lon.flatten(), grid_lat.flatten()), axis=1)\n",
    "\n",
    "    # Obtain predictions, means, and stddevs over the entire map\n",
    "    predictions, gp_mean, gp_stddev = model.make_predictions(visualization_xs)\n",
    "    predictions = np.reshape(predictions, (EVALUATION_GRID_POINTS, EVALUATION_GRID_POINTS))\n",
    "    gp_mean = np.reshape(gp_mean, (EVALUATION_GRID_POINTS, EVALUATION_GRID_POINTS))\n",
    "    gp_stddev = np.reshape(gp_stddev, (EVALUATION_GRID_POINTS, EVALUATION_GRID_POINTS))\n",
    "\n",
    "    vmin, vmax = 0.0, 65.0\n",
    "    vmax_stddev = 35.5\n",
    "\n",
    "    # Plot the actual predictions\n",
    "    ax_predictions = fig.add_subplot(1, 3, 1)\n",
    "    predictions_plot = ax_predictions.imshow(predictions, vmin=vmin, vmax=vmax)\n",
    "    ax_predictions.set_title('Predictions')\n",
    "    fig.colorbar(predictions_plot)\n",
    "\n",
    "    # Plot the raw GP predictions with their stddeviations\n",
    "    ax_gp = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "    ax_gp.plot_surface(\n",
    "        X=grid_lon,\n",
    "        Y=grid_lat,\n",
    "        Z=gp_mean,\n",
    "        facecolors=cm.get_cmap()(gp_stddev / vmax_stddev),\n",
    "        rcount=EVALUATION_GRID_POINTS_3D,\n",
    "        ccount=EVALUATION_GRID_POINTS_3D,\n",
    "        linewidth=0,\n",
    "        antialiased=False\n",
    "    )\n",
    "    ax_gp.set_zlim(vmin, vmax)\n",
    "    ax_gp.set_title('GP means, colors are GP stddev')\n",
    "\n",
    "    # Plot the standard deviations\n",
    "    ax_stddev = fig.add_subplot(1, 3, 3)\n",
    "    stddev_plot = ax_stddev.imshow(gp_stddev, vmin=vmin, vmax=vmax_stddev)\n",
    "    ax_stddev.set_title('GP estimated stddev')\n",
    "    fig.colorbar(stddev_plot)\n",
    "\n",
    "    # Save figure to pdf\n",
    "    figure_path = os.path.join(output_dir, 'extended_evaluation.pdf')\n",
    "    fig.savefig(figure_path)\n",
    "    print(f'Saved extended evaluation to {figure_path}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0feacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape: torch.Size([13670, 2])\n",
      "train_y.shape: torch.Size([13670])\n",
      "val_X.shape: torch.Size([1519, 2])\n",
      "val_y.shape: torch.Size([1519])\n",
      "Fitting model\n",
      "Iteration %d/%d - Loss: %.3f (0, 240, 171.50177001953125)\n",
      "Iteration %d/%d - Loss: %.3f (1, 240, 112.4499282836914)\n",
      "Iteration %d/%d - Loss: %.3f (2, 240, 77.46973419189453)\n",
      "Iteration %d/%d - Loss: %.3f (3, 240, 52.67660903930664)\n",
      "Iteration %d/%d - Loss: %.3f (4, 240, 33.23040008544922)\n",
      "Iteration %d/%d - Loss: %.3f (5, 240, 20.206703186035156)\n",
      "Iteration %d/%d - Loss: %.3f (6, 240, 11.511824607849121)\n",
      "Iteration %d/%d - Loss: %.3f (7, 240, 6.850081920623779)\n",
      "Iteration %d/%d - Loss: %.3f (8, 240, 5.7935943603515625)\n",
      "Iteration %d/%d - Loss: %.3f (9, 240, 6.852457046508789)\n",
      "Iteration %d/%d - Loss: %.3f (10, 240, 8.807466506958008)\n",
      "Iteration %d/%d - Loss: %.3f (11, 240, 10.69225025177002)\n",
      "Iteration %d/%d - Loss: %.3f (12, 240, 11.6947021484375)\n",
      "Iteration %d/%d - Loss: %.3f (13, 240, 11.527063369750977)\n",
      "Iteration %d/%d - Loss: %.3f (14, 240, 10.448843002319336)\n",
      "Iteration %d/%d - Loss: %.3f (15, 240, 8.921125411987305)\n",
      "Iteration %d/%d - Loss: %.3f (16, 240, 7.374162197113037)\n",
      "Iteration %d/%d - Loss: %.3f (17, 240, 6.043076992034912)\n",
      "Iteration %d/%d - Loss: %.3f (18, 240, 5.0260515213012695)\n",
      "Iteration %d/%d - Loss: %.3f (19, 240, 4.304840087890625)\n",
      "Iteration %d/%d - Loss: %.3f (20, 240, 3.8405284881591797)\n",
      "Iteration %d/%d - Loss: %.3f (21, 240, 3.569538116455078)\n",
      "Iteration %d/%d - Loss: %.3f (22, 240, 3.452120304107666)\n",
      "Iteration %d/%d - Loss: %.3f (23, 240, 3.4425792694091797)\n",
      "Iteration %d/%d - Loss: %.3f (24, 240, 3.515812397003174)\n",
      "Iteration %d/%d - Loss: %.3f (25, 240, 3.6371662616729736)\n",
      "Iteration %d/%d - Loss: %.3f (26, 240, 3.777470111846924)\n",
      "Iteration %d/%d - Loss: %.3f (27, 240, 3.902146577835083)\n",
      "Iteration %d/%d - Loss: %.3f (28, 240, 3.990816116333008)\n",
      "Iteration %d/%d - Loss: %.3f (29, 240, 4.021816253662109)\n",
      "Iteration %d/%d - Loss: %.3f (30, 240, 3.9970951080322266)\n",
      "Iteration %d/%d - Loss: %.3f (31, 240, 3.9116742610931396)\n",
      "Iteration %d/%d - Loss: %.3f (32, 240, 3.8072001934051514)\n",
      "Iteration %d/%d - Loss: %.3f (33, 240, 3.673607349395752)\n",
      "Iteration %d/%d - Loss: %.3f (34, 240, 3.5435731410980225)\n",
      "Iteration %d/%d - Loss: %.3f (35, 240, 3.4156575202941895)\n",
      "Iteration %d/%d - Loss: %.3f (36, 240, 3.3118481636047363)\n",
      "Iteration %d/%d - Loss: %.3f (37, 240, 3.2346436977386475)\n",
      "Iteration %d/%d - Loss: %.3f (38, 240, 3.1716887950897217)\n",
      "Iteration %d/%d - Loss: %.3f (39, 240, 3.138007402420044)\n",
      "Iteration %d/%d - Loss: %.3f (40, 240, 3.1154658794403076)\n",
      "Iteration %d/%d - Loss: %.3f (41, 240, 3.1135997772216797)\n",
      "Iteration %d/%d - Loss: %.3f (42, 240, 3.1147279739379883)\n",
      "Iteration %d/%d - Loss: %.3f (43, 240, 3.1251909732818604)\n",
      "Iteration %d/%d - Loss: %.3f (44, 240, 3.133951187133789)\n",
      "Iteration %d/%d - Loss: %.3f (45, 240, 3.142193555831909)\n",
      "Iteration %d/%d - Loss: %.3f (46, 240, 3.1525325775146484)\n",
      "Iteration %d/%d - Loss: %.3f (47, 240, 3.1529603004455566)\n",
      "Iteration %d/%d - Loss: %.3f (48, 240, 3.1500539779663086)\n",
      "Iteration %d/%d - Loss: %.3f (49, 240, 3.144209146499634)\n",
      "Iteration %d/%d - Loss: %.3f (50, 240, 3.133772611618042)\n",
      "Iteration %d/%d - Loss: %.3f (51, 240, 3.119136095046997)\n",
      "Iteration %d/%d - Loss: %.3f (52, 240, 3.108523368835449)\n",
      "Iteration %d/%d - Loss: %.3f (53, 240, 3.0985524654388428)\n",
      "Iteration %d/%d - Loss: %.3f (54, 240, 3.084127902984619)\n",
      "Iteration %d/%d - Loss: %.3f (55, 240, 3.072197914123535)\n",
      "Iteration %d/%d - Loss: %.3f (56, 240, 3.0610148906707764)\n",
      "Iteration %d/%d - Loss: %.3f (57, 240, 3.0581865310668945)\n",
      "Iteration %d/%d - Loss: %.3f (58, 240, 3.0553061962127686)\n",
      "Iteration %d/%d - Loss: %.3f (59, 240, 3.057326316833496)\n",
      "Iteration %d/%d - Loss: %.3f (60, 240, 3.053980588912964)\n",
      "Iteration %d/%d - Loss: %.3f (61, 240, 3.0609772205352783)\n",
      "Iteration %d/%d - Loss: %.3f (62, 240, 3.060668706893921)\n",
      "Iteration %d/%d - Loss: %.3f (63, 240, 3.0654594898223877)\n",
      "Iteration %d/%d - Loss: %.3f (64, 240, 3.068192720413208)\n",
      "Iteration %d/%d - Loss: %.3f (65, 240, 3.066697359085083)\n",
      "Iteration %d/%d - Loss: %.3f (66, 240, 3.066833972930908)\n",
      "Iteration %d/%d - Loss: %.3f (67, 240, 3.0670294761657715)\n",
      "Iteration %d/%d - Loss: %.3f (68, 240, 3.0616612434387207)\n",
      "Iteration %d/%d - Loss: %.3f (69, 240, 3.0580689907073975)\n",
      "Iteration %d/%d - Loss: %.3f (70, 240, 3.054586887359619)\n",
      "Iteration %d/%d - Loss: %.3f (71, 240, 3.052489995956421)\n",
      "Iteration %d/%d - Loss: %.3f (72, 240, 3.0509088039398193)\n",
      "Iteration %d/%d - Loss: %.3f (73, 240, 3.0426828861236572)\n",
      "Iteration %d/%d - Loss: %.3f (74, 240, 3.034651041030884)\n",
      "Iteration %d/%d - Loss: %.3f (75, 240, 3.036395311355591)\n",
      "Iteration %d/%d - Loss: %.3f (76, 240, 3.0381603240966797)\n",
      "Iteration %d/%d - Loss: %.3f (77, 240, 3.03246808052063)\n",
      "Iteration %d/%d - Loss: %.3f (78, 240, 3.0273635387420654)\n",
      "Iteration %d/%d - Loss: %.3f (79, 240, 3.0259644985198975)\n",
      "Iteration %d/%d - Loss: %.3f (80, 240, 3.028029441833496)\n",
      "Iteration %d/%d - Loss: %.3f (81, 240, 3.0218067169189453)\n",
      "Iteration %d/%d - Loss: %.3f (82, 240, 3.0250511169433594)\n",
      "Iteration %d/%d - Loss: %.3f (83, 240, 3.024327039718628)\n",
      "Iteration %d/%d - Loss: %.3f (84, 240, 3.025834798812866)\n",
      "Iteration %d/%d - Loss: %.3f (85, 240, 3.020930051803589)\n",
      "Iteration %d/%d - Loss: %.3f (86, 240, 3.0158233642578125)\n",
      "Iteration %d/%d - Loss: %.3f (87, 240, 3.014732837677002)\n",
      "Iteration %d/%d - Loss: %.3f (88, 240, 3.020326852798462)\n",
      "Iteration %d/%d - Loss: %.3f (89, 240, 3.017397165298462)\n",
      "Iteration %d/%d - Loss: %.3f (90, 240, 3.012617349624634)\n",
      "Iteration %d/%d - Loss: %.3f (91, 240, 3.0154941082000732)\n",
      "Iteration %d/%d - Loss: %.3f (92, 240, 3.012550115585327)\n",
      "Iteration %d/%d - Loss: %.3f (93, 240, 3.0077250003814697)\n",
      "Iteration %d/%d - Loss: %.3f (94, 240, 3.0099070072174072)\n",
      "Iteration %d/%d - Loss: %.3f (95, 240, 3.0049312114715576)\n",
      "Iteration %d/%d - Loss: %.3f (96, 240, 3.007110357284546)\n",
      "Iteration %d/%d - Loss: %.3f (97, 240, 3.0048391819000244)\n",
      "Iteration %d/%d - Loss: %.3f (98, 240, 3.0041518211364746)\n",
      "Iteration %d/%d - Loss: %.3f (99, 240, 3.001561403274536)\n",
      "Iteration %d/%d - Loss: %.3f (100, 240, 2.9974205493927)\n",
      "Iteration %d/%d - Loss: %.3f (101, 240, 2.9971680641174316)\n",
      "Iteration %d/%d - Loss: %.3f (102, 240, 3.0006840229034424)\n",
      "Iteration %d/%d - Loss: %.3f (103, 240, 2.9983999729156494)\n",
      "Iteration %d/%d - Loss: %.3f (104, 240, 3.0031213760375977)\n",
      "Iteration %d/%d - Loss: %.3f (105, 240, 2.988828659057617)\n",
      "Iteration %d/%d - Loss: %.3f (106, 240, 2.9912495613098145)\n",
      "Iteration %d/%d - Loss: %.3f (107, 240, 2.991848945617676)\n",
      "Iteration %d/%d - Loss: %.3f (108, 240, 2.9907047748565674)\n",
      "Iteration %d/%d - Loss: %.3f (109, 240, 2.989797353744507)\n",
      "Iteration %d/%d - Loss: %.3f (110, 240, 2.9859557151794434)\n",
      "Iteration %d/%d - Loss: %.3f (111, 240, 2.985612392425537)\n",
      "Iteration %d/%d - Loss: %.3f (112, 240, 2.9839680194854736)\n",
      "Iteration %d/%d - Loss: %.3f (113, 240, 2.9858272075653076)\n",
      "Iteration %d/%d - Loss: %.3f (114, 240, 2.9841694831848145)\n",
      "Iteration %d/%d - Loss: %.3f (115, 240, 2.9765806198120117)\n",
      "Iteration %d/%d - Loss: %.3f (116, 240, 2.9708411693573)\n",
      "Iteration %d/%d - Loss: %.3f (117, 240, 2.971736431121826)\n",
      "Iteration %d/%d - Loss: %.3f (118, 240, 2.974583625793457)\n",
      "Iteration %d/%d - Loss: %.3f (119, 240, 2.9661660194396973)\n",
      "Iteration %d/%d - Loss: %.3f (120, 240, 2.9695818424224854)\n",
      "Iteration %d/%d - Loss: %.3f (121, 240, 2.967973232269287)\n",
      "Iteration %d/%d - Loss: %.3f (122, 240, 2.9656646251678467)\n",
      "Iteration %d/%d - Loss: %.3f (123, 240, 2.9641716480255127)\n",
      "Iteration %d/%d - Loss: %.3f (124, 240, 2.9667179584503174)\n",
      "Iteration %d/%d - Loss: %.3f (125, 240, 2.9596550464630127)\n",
      "Iteration %d/%d - Loss: %.3f (126, 240, 2.9633584022521973)\n",
      "Iteration %d/%d - Loss: %.3f (127, 240, 2.959188461303711)\n",
      "Iteration %d/%d - Loss: %.3f (128, 240, 2.9542150497436523)\n",
      "Iteration %d/%d - Loss: %.3f (129, 240, 2.9559073448181152)\n",
      "Iteration %d/%d - Loss: %.3f (130, 240, 2.951556444168091)\n",
      "Iteration %d/%d - Loss: %.3f (131, 240, 2.9459383487701416)\n",
      "Iteration %d/%d - Loss: %.3f (132, 240, 2.9540936946868896)\n",
      "Iteration %d/%d - Loss: %.3f (133, 240, 2.94558048248291)\n",
      "Iteration %d/%d - Loss: %.3f (134, 240, 2.9492452144622803)\n",
      "Iteration %d/%d - Loss: %.3f (135, 240, 2.9432857036590576)\n",
      "Iteration %d/%d - Loss: %.3f (136, 240, 2.947460889816284)\n",
      "Iteration %d/%d - Loss: %.3f (137, 240, 2.9471139907836914)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration %d/%d - Loss: %.3f (138, 240, 2.9411087036132812)\n",
      "Iteration %d/%d - Loss: %.3f (139, 240, 2.938007116317749)\n",
      "Iteration %d/%d - Loss: %.3f (140, 240, 2.936422109603882)\n",
      "Iteration %d/%d - Loss: %.3f (141, 240, 2.9411072731018066)\n",
      "Iteration %d/%d - Loss: %.3f (142, 240, 2.937609910964966)\n",
      "Iteration %d/%d - Loss: %.3f (143, 240, 2.93274188041687)\n",
      "Iteration %d/%d - Loss: %.3f (144, 240, 2.93212890625)\n",
      "Iteration %d/%d - Loss: %.3f (145, 240, 2.9397716522216797)\n",
      "Iteration %d/%d - Loss: %.3f (146, 240, 2.9319117069244385)\n",
      "Iteration %d/%d - Loss: %.3f (147, 240, 2.9263415336608887)\n",
      "Iteration %d/%d - Loss: %.3f (148, 240, 2.9315743446350098)\n",
      "Iteration %d/%d - Loss: %.3f (149, 240, 2.926771402359009)\n",
      "Iteration %d/%d - Loss: %.3f (150, 240, 2.9238712787628174)\n",
      "Iteration %d/%d - Loss: %.3f (151, 240, 2.9207279682159424)\n",
      "Iteration %d/%d - Loss: %.3f (152, 240, 2.921353816986084)\n",
      "Iteration %d/%d - Loss: %.3f (153, 240, 2.921623468399048)\n",
      "Iteration %d/%d - Loss: %.3f (154, 240, 2.9204964637756348)\n",
      "Iteration %d/%d - Loss: %.3f (155, 240, 2.921895742416382)\n",
      "Iteration %d/%d - Loss: %.3f (156, 240, 2.9182045459747314)\n",
      "Iteration %d/%d - Loss: %.3f (157, 240, 2.916008710861206)\n",
      "Iteration %d/%d - Loss: %.3f (158, 240, 2.911949634552002)\n",
      "Iteration %d/%d - Loss: %.3f (159, 240, 2.9165968894958496)\n",
      "Iteration %d/%d - Loss: %.3f (160, 240, 2.912233352661133)\n",
      "Iteration %d/%d - Loss: %.3f (161, 240, 2.9030327796936035)\n",
      "Iteration %d/%d - Loss: %.3f (162, 240, 2.9061505794525146)\n",
      "Iteration %d/%d - Loss: %.3f (163, 240, 2.9081385135650635)\n",
      "Iteration %d/%d - Loss: %.3f (164, 240, 2.907135248184204)\n",
      "Iteration %d/%d - Loss: %.3f (165, 240, 2.9021363258361816)\n",
      "Iteration %d/%d - Loss: %.3f (166, 240, 2.9047670364379883)\n",
      "Iteration %d/%d - Loss: %.3f (167, 240, 2.901808023452759)\n",
      "Iteration %d/%d - Loss: %.3f (168, 240, 2.896137237548828)\n",
      "Iteration %d/%d - Loss: %.3f (169, 240, 2.8969738483428955)\n",
      "Iteration %d/%d - Loss: %.3f (170, 240, 2.899850368499756)\n",
      "Iteration %d/%d - Loss: %.3f (171, 240, 2.8952581882476807)\n",
      "Iteration %d/%d - Loss: %.3f (172, 240, 2.8918278217315674)\n",
      "Iteration %d/%d - Loss: %.3f (173, 240, 2.9009571075439453)\n",
      "Iteration %d/%d - Loss: %.3f (174, 240, 2.892657518386841)\n",
      "Iteration %d/%d - Loss: %.3f (175, 240, 2.898367404937744)\n",
      "Iteration %d/%d - Loss: %.3f (176, 240, 2.8899495601654053)\n",
      "Iteration %d/%d - Loss: %.3f (177, 240, 2.894739866256714)\n",
      "Iteration %d/%d - Loss: %.3f (178, 240, 2.8841750621795654)\n",
      "Iteration %d/%d - Loss: %.3f (179, 240, 2.8853886127471924)\n",
      "Iteration %d/%d - Loss: %.3f (180, 240, 2.8857064247131348)\n",
      "Iteration %d/%d - Loss: %.3f (181, 240, 2.887747287750244)\n",
      "Iteration %d/%d - Loss: %.3f (182, 240, 2.880007028579712)\n",
      "Iteration %d/%d - Loss: %.3f (183, 240, 2.885573148727417)\n",
      "Iteration %d/%d - Loss: %.3f (184, 240, 2.881234884262085)\n",
      "Iteration %d/%d - Loss: %.3f (185, 240, 2.88295841217041)\n",
      "Iteration %d/%d - Loss: %.3f (186, 240, 2.885226249694824)\n",
      "Iteration %d/%d - Loss: %.3f (187, 240, 2.8794360160827637)\n",
      "Iteration %d/%d - Loss: %.3f (188, 240, 2.8756444454193115)\n",
      "Iteration %d/%d - Loss: %.3f (189, 240, 2.883025646209717)\n",
      "Iteration %d/%d - Loss: %.3f (190, 240, 2.877154588699341)\n",
      "Iteration %d/%d - Loss: %.3f (191, 240, 2.8739144802093506)\n",
      "Iteration %d/%d - Loss: %.3f (192, 240, 2.8720412254333496)\n",
      "Iteration %d/%d - Loss: %.3f (193, 240, 2.8754773139953613)\n",
      "Iteration %d/%d - Loss: %.3f (194, 240, 2.877657175064087)\n",
      "Iteration %d/%d - Loss: %.3f (195, 240, 2.8695547580718994)\n",
      "Iteration %d/%d - Loss: %.3f (196, 240, 2.8739383220672607)\n",
      "Iteration %d/%d - Loss: %.3f (197, 240, 2.8719513416290283)\n",
      "Iteration %d/%d - Loss: %.3f (198, 240, 2.8680760860443115)\n",
      "Iteration %d/%d - Loss: %.3f (199, 240, 2.8650643825531006)\n",
      "Iteration %d/%d - Loss: %.3f (200, 240, 2.869185209274292)\n",
      "Iteration %d/%d - Loss: %.3f (201, 240, 2.865438222885132)\n",
      "Iteration %d/%d - Loss: %.3f (202, 240, 2.8657453060150146)\n",
      "Iteration %d/%d - Loss: %.3f (203, 240, 2.8678760528564453)\n",
      "Iteration %d/%d - Loss: %.3f (204, 240, 2.8576767444610596)\n",
      "Iteration %d/%d - Loss: %.3f (205, 240, 2.863797426223755)\n",
      "Iteration %d/%d - Loss: %.3f (206, 240, 2.865950584411621)\n",
      "Iteration %d/%d - Loss: %.3f (207, 240, 2.8648087978363037)\n",
      "Iteration %d/%d - Loss: %.3f (208, 240, 2.860034227371216)\n",
      "Iteration %d/%d - Loss: %.3f (209, 240, 2.8624267578125)\n",
      "Iteration %d/%d - Loss: %.3f (210, 240, 2.853351354598999)\n",
      "Iteration %d/%d - Loss: %.3f (211, 240, 2.85385799407959)\n",
      "Iteration %d/%d - Loss: %.3f (212, 240, 2.857560157775879)\n",
      "Iteration %d/%d - Loss: %.3f (213, 240, 2.8560659885406494)\n",
      "Iteration %d/%d - Loss: %.3f (214, 240, 2.8560197353363037)\n",
      "Iteration %d/%d - Loss: %.3f (215, 240, 2.8568012714385986)\n",
      "Iteration %d/%d - Loss: %.3f (216, 240, 2.8499834537506104)\n",
      "Iteration %d/%d - Loss: %.3f (217, 240, 2.856515407562256)\n",
      "Iteration %d/%d - Loss: %.3f (218, 240, 2.8539621829986572)\n",
      "Iteration %d/%d - Loss: %.3f (219, 240, 2.8518364429473877)\n",
      "Iteration %d/%d - Loss: %.3f (220, 240, 2.849384307861328)\n",
      "Iteration %d/%d - Loss: %.3f (221, 240, 2.849372386932373)\n",
      "Iteration %d/%d - Loss: %.3f (222, 240, 2.851015329360962)\n",
      "Iteration %d/%d - Loss: %.3f (223, 240, 2.8513426780700684)\n",
      "Iteration %d/%d - Loss: %.3f (224, 240, 2.8492448329925537)\n",
      "Iteration %d/%d - Loss: %.3f (225, 240, 2.8489720821380615)\n",
      "Iteration %d/%d - Loss: %.3f (226, 240, 2.8463473320007324)\n",
      "Iteration %d/%d - Loss: %.3f (227, 240, 2.8508355617523193)\n",
      "Iteration %d/%d - Loss: %.3f (228, 240, 2.848372459411621)\n",
      "Iteration %d/%d - Loss: %.3f (229, 240, 2.845858335494995)\n",
      "Iteration %d/%d - Loss: %.3f (230, 240, 2.8458642959594727)\n",
      "Iteration %d/%d - Loss: %.3f (231, 240, 2.843177318572998)\n",
      "Iteration %d/%d - Loss: %.3f (232, 240, 2.8439130783081055)\n",
      "Iteration %d/%d - Loss: %.3f (233, 240, 2.8418803215026855)\n",
      "Iteration %d/%d - Loss: %.3f (234, 240, 2.8455193042755127)\n",
      "Iteration %d/%d - Loss: %.3f (235, 240, 2.848423957824707)\n",
      "Iteration %d/%d - Loss: %.3f (236, 240, 2.845412015914917)\n",
      "Iteration %d/%d - Loss: %.3f (237, 240, 2.8422722816467285)\n",
      "Iteration %d/%d - Loss: %.3f (238, 240, 2.8357760906219482)\n",
      "Iteration %d/%d - Loss: %.3f (239, 240, 2.837954044342041)\n",
      "Cost of evaluation set: %.3f 50.23375\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the training dateset and test features\n",
    "    train_features = np.loadtxt('train_x.csv', delimiter=',', skiprows=1)\n",
    "    train_GT = np.loadtxt('train_y.csv', delimiter=',', skiprows=1)\n",
    "    test_features = np.loadtxt('test_x.csv', delimiter=',', skiprows=1)\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(train_features,train_GT, \n",
    "                                                      train_size=0.9,test_size=0.1,\n",
    "                                                      random_state=0,shuffle=True)\n",
    "    \n",
    "    train_X = torch.Tensor(train_X)\n",
    "    train_Y = torch.Tensor(train_Y).squeeze()\n",
    "\n",
    "    val_X = torch.Tensor(val_X)\n",
    "    val_Y = torch.Tensor(val_Y).squeeze()\n",
    "    \n",
    "    print(f\"train_X.shape: {train_X.shape}\")\n",
    "    print(f\"train_y.shape: {train_Y.shape}\")\n",
    "    print(f\"val_X.shape: {val_X.shape}\")\n",
    "    print(f\"val_y.shape: {val_Y.shape}\")\n",
    "    \n",
    "    # Fit the model\n",
    "    print('Fitting model')\n",
    "    \n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = Model(train_X, train_Y, likelihood)\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.5)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    iterations = 240\n",
    "    for i in range(iterations):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_X)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_Y)\n",
    "        loss.backward()\n",
    "        print('Iteration %d/%d - Loss: %.3f' , (i, iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        f_x = model(val_X)\n",
    "        mean = f_x.mean\n",
    "        f_x_lower, f_x_upper = f_x.confidence_region()\n",
    "        y = likelihood(f_x)\n",
    "        y_lower, y_upper = y.confidence_region()   \n",
    "    cost = cost_function(val_Y.cpu().numpy(), mean.cpu().numpy())\n",
    "    print(\"Cost of evaluation set: %.3f\",cost)\n",
    "\n",
    "    #model = Model(train_features,train_GT,likelihood)\n",
    "    #fitting_model(model,train_GT,train_features)\n",
    "\n",
    "    # Predict on the test features\n",
    "    #print('Predicting on test features')\n",
    "    #predictions, gp_mean, gp_std = model.make_predictions(test_features)\n",
    "    \n",
    "    #print(predictions)\n",
    "    \n",
    "    #cost_function(train_GT, predictions)\n",
    "\n",
    "    #if EXTENDED_EVALUATION:\n",
    "    #    perform_extended_evaluation(model, output_dir='.')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44fec90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b2861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
